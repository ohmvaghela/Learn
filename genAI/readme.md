# Gen AI
- Data generated by AI
- LLM : Large Language Model
- `LLM` are part of `Foundation Models`
- `Foundation Models` are part of `Generative AI`

## Hierarchy
- AL
	- ML
		- NN
			- DL
				- GenAI

### Machine Learning
- Training algo to learn from data 
	- and make predictions
- ML include supervised, unsupervised and reinforcement learning(AI trained to make optimal decisions)

### Neural Network 
- ML model inspired from structure of brain
- Consist of interconnected nodes (neurons) that each `process and transform data` to meaningful output
- Can be trained to perform tasks like classification, regression(predict continous output like stocks), feature learning(dicover new features)
- 

### DL (Deep Learning)
- ML model that focuses on NN with more then two layers
- Used to learn complex patterns and representation in data like images, speech, human text
- It has CNN, RNN etc

## Types of ML

### Supervised Learning
- Involved training on labeled data where correct output is known 
- Goal is to learn mapping between input and output
- Algorithms Examples
	- Linear Regression
	- Logistic Regression
	- Decision Trees
### Unsupervised learning
- Involves training data on unlabeled data
- Goal is to find patterns, relations, structure
- Eg. algo
	- Clustering
	- Dimensionality reduction
	- K-Means Clustering
	- PCA
### Semi-supervised learning
- Training a model on a combination of labeled and unlabeled data.
- Goal is to leverage labeled data for good representation and use unlabeled data to improve model's perfromance
- Eg
	-	Self-training
	- Co-training
	- Label propagation
	- Graph-based methods
### Reinforcement Learning
- Involves agent to take actions in the enviroment to maximize reward
- like say I am playing a game and the actions I take in the envt. lead to reward, obstacle etc.
	- and my aim to maximize the rewards 
- Usecase
	- Robotic manuplator
	- Autonomous navigation

# ML algos

## Linear regression
- It is a supervised machine learning algo
- It tries to fit model on a single line 
- It assumes relation between input and output is linear
$$ y(x) = a*x(i) + b $$

## Logistic regression
- Used to model probability of event, that what is probability of event to occur
	- Then we can define threshold if the probability if greater then it is Yes, no otherwise
$$
f(x) = \frac{1}{1+ e^{b_{1}*x + b_{o}}}
$$

## Decision Tree
- Used for classification and regression task
- It splits down based on decision to come to consulsion
- It has following nodes
	- Root node : Input is given here
	- Decision Node : intermediate node, splits data based on specific attribute or features
	- Leaf node : The bottom most node, has the final value of prediction or decision
- Other terminologis
	- Spliting : Proccess of dividing nodes to two or more sub nodes
	- Pruning : Removing sub-nodes of decision tree
- Gini Impurity:
	- Determine quality of split
	- Probability of misclassifying dataset for current instance
- If group impurity or impurity at node goes to zero then we stop splitting 

### Random forest 
- Colleection of multiple decision trees put together to improve accuracy and roburstness of predictions
- Working:
	1. Bootstrap sample : Randomly selecting subset of data for training
	2. Decision tree creation : Creating decision tree using the bootstrap sample
	3. Feature Recognition : Randomly select a subset of features to consider for each decision node
	4. Prediction : Make prediction using Decision tree
	5. Repeat : 1-4 steps multiple times
	6. Combine Predictions to make final predictions

### Support Vector Machine (SVM)
- Supervised learning algo
- Used for classification
- SVM does this by creating hyperline(2D) / hyperplane(3D) to seperate data
- Support Vector : Points closest points to hyperplane that are used to create hyperplane
- Margin: Distance between the hyperplane and support vectors
- Kernel: Function to transform data into higher dimensions
- Types :
	-	Linear SVM : Using linear kernel
	- Non-Linear SVM : Using non-linear kernel 
	- Soft margin SVM : Allows for some missclassification to allow generalisation

### Naive bayes
- Supervised learning algo
- Probability v/s Conditional Probability
	- Probability : Chances of occurance of a events
	- Conditional Probability : Chances of occurance second event given first event has already occured
- It assumes features are independent of each other

### K-nearest Neighbour (KNN)
- Supervised Learning algo
- used for Classification and regresion task
- It uses a variable k to find nearest neighbour to the data point 
	- Then it classifies the data point as the one which has most neighbour
	- So we need to select K properly

### PCA
- Dimension reduction technique
- Transform higher dimension data to lower dimension while retaining most of info
- Unsupervised learning technique

### K-Mean
- K : Number of cluster we want to classify data into
- Unsupervied learning 
- Used for dividing data into groups and minimize within-cluster variance

# Activation functions
- Mathematical functions applied to neuron output
	- It is applied to decide wether to activate the neuron or not
- It introduces non linearity to the model
	- This helps model to learn and represent complex data
- If not used then the model will converge to linear regression 
- SomeTypes : 

### Step Functions:
- If value is greater then threshold then neuron will be activated
$$

f(x) =
\begin{cases}
0  \text{ if } x < \theta  \\
1  \text{ if } x > \theta
\end{cases}

$$

### Sigmoid
- Most used 
- Output in range 0-1
$$
f(x) = \frac{1}{1+e^{-x}}
$$

### TanH
- Range -1 to +1
- Good choise for hidden layer
$$
f(x) = \frac{2}{1+e^{-2x}} - 1
$$

### ReLU
- Also a popular choise for hidden layer
- But after prolong use it can give 0 always so leaky ReLU is used sometimes
$$
f(x) = max(0,x)
$$

### Leaky ReLU
$$
f(x) = 
\begin{cases}
x \text{ if } x >= 0 \\
a.x \text{ if } x < 0 
\end{cases}
$$
- Here a is very small value in range of 0.001

### Softmax
- Outputs btwn 0-1 like a probability
- Usually used in last layer
$$
S(x_{i}) = \frac{e^{x_{i}}}{\sum e^{x_{i}}}
$$


# Neural network in depth (Or Artificial Neural Network)
- There are layers of neurons 
- If number of layers surpass 2 we called it Deep Neural Network 
	- This is base of Deep learning
## CNN (Convolution Neural Network)
- Used in Image recognition
- It passes through first few convolution layer that apply filters to input image to detect features
- Then some processing is done

## RNN (Recurrent Neural Network)
- Used in Audio/Temporal files
- We can have feedback loop
	- Feedback loops : Running the output again to same neuron or pervious neurons

## Autoencoder
- Take high dimensional signal --> Compress it --> relift it to high dimensional signal
- PCA is an example


# Machine Learning model deployment
- https://www.youtube.com/watch?v=SHyFjJ-tIJE


